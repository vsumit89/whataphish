{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as BeautifulSoup\n",
    "import os\n",
    "import requests\n",
    "from subprocess import *\n",
    "import json\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: python-whois in /home/sumit/.local/lib/python3.8/site-packages (0.7.3)\n",
      "Requirement already satisfied: future in /usr/lib/python3/dist-packages (from python-whois) (0.18.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install python-whois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "import favicon\n",
    "import xml.etree.ElementTree as ET \n",
    "import tldextract\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import whois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import signal\n",
    "\n",
    "class TimeoutException(Exception):   # Custom exception class\n",
    "    pass\n",
    "\n",
    "def timeout_handler(signum, frame):   # Custom signal handler\n",
    "    raise TimeoutException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_rightclick(url):\n",
    "  try:\n",
    "    html_content = requests.get(url).text\n",
    "    soup = BeautifulSoup.BeautifulSoup(html_content, \"lxml\")\n",
    "    if str(soup).lower().find(\"preventdefault()\") != -1:\n",
    "      return -1\n",
    "    elif str(soup).lower().find(\"event.button==2\") != -1:\n",
    "      return -1\n",
    "    elif str(soup).lower().find(\"event.button == 2\") != -1:\n",
    "      return -1\n",
    "    return 1\n",
    "  except:\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "flag = check_rightclick(\"http://www.instagram.com\")\n",
    "print(flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_rightclick(\"https://stackoverflow.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_find_having_ip_add(url):\n",
    "  import string\n",
    "  index = url.find(\"://\")\n",
    "  split_url = url[index+3:]\n",
    "  index = split_url.find(\"/\")\n",
    "  split_url = split_url[:index]\n",
    "  split_url = split_url.replace(\".\", \"\")\n",
    "  counter_hex = 0\n",
    "  for i in split_url:\n",
    "    if i in string.hexdigits:\n",
    "      counter_hex +=1\n",
    "\n",
    "  total_len = len(split_url)\n",
    "  having_IP_Address = 1\n",
    "  if counter_hex >= total_len:\n",
    "    having_IP_Address = -1\n",
    "\n",
    "  return having_IP_Address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_find_url_len(url):\n",
    "  URL_Length = 1\n",
    "  if len(url)>=75:\n",
    "    URL_Length = -1\n",
    "  elif len(url)>=54 and len(url)<=74:\n",
    "    URL_length = 0\n",
    "  \n",
    "  return URL_Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_complete_URL(shortened_url):\n",
    "  command_stdout = Popen(['curl', shortened_url], stdout=PIPE).communicate()[0]\n",
    "  output = command_stdout.decode('utf-8')\n",
    "  href_index = output.find(\"href=\")\n",
    "  if href_index == -1:\n",
    "    href_index = output.find(\"HREF=\")\n",
    "  splitted_ = output[href_index:].split('\"')\n",
    "  expanded_url = splitted_[1]\n",
    "  return expanded_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_shortened_url(url):\n",
    "  famous_short_urls = [\"bit.ly\", \"tinyurl.com\", \"goo.gl\",\n",
    "                       \"rebrand.ly\", \"t.co\", \"youtu.be\",\n",
    "                       \"ow.ly\", \"w.wiki\", \"is.gd\"]\n",
    "\n",
    "  domain_of_url = url.split(\"://\")[1]\n",
    "  domain_of_url = domain_of_url.split(\"/\")[0]\n",
    "  status = 1\n",
    "  if domain_of_url in famous_short_urls:\n",
    "    status = -1\n",
    "\n",
    "  complete_url = None\n",
    "  if status == -1:\n",
    "    complete_url = get_complete_URL(url)\n",
    "\n",
    "  return (status, complete_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_find_at(url):\n",
    "  label = 1\n",
    "  index = url.find(\"@\")\n",
    "  if index!=-1:\n",
    "    label = -1\n",
    "  \n",
    "  return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_find_redirect(url):\n",
    "  index = url.find(\"://\")\n",
    "  split_url = url[index+3:]\n",
    "  label = 1\n",
    "  index = split_url.find(\"//\")\n",
    "  if index!=-1:\n",
    "    label = -1\n",
    "  \n",
    "  return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_find_prefix(url):\n",
    "  index = url.find(\"://\")\n",
    "  split_url = url[index+3:]\n",
    "  index = split_url.find(\"/\")\n",
    "  split_url = split_url[:index]\n",
    "  label = 1\n",
    "  index = split_url.find(\"-\")\n",
    "  if index!=-1:\n",
    "    label = -1\n",
    "  \n",
    "  return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_find_multi_domains(url):\n",
    "  url = url.split(\"://\")[1]\n",
    "  url = url.split(\"/\")[0]\n",
    "  index = url.find(\"www.\")\n",
    "  split_url = url\n",
    "  if index!=-1:\n",
    "    split_url = url[index+4:]\n",
    "  index = split_url.rfind(\".\")\n",
    "  if index!=-1:\n",
    "    split_url = split_url[:index]\n",
    "  counter = 0\n",
    "  for i in split_url:\n",
    "    if i==\".\":\n",
    "      counter+=1\n",
    "  \n",
    "  label = 1\n",
    "  if counter==2:\n",
    "    label = 0\n",
    "  elif counter >=3:\n",
    "    label = -1\n",
    "  \n",
    "  return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_find_authority(url):\n",
    "  index_https = url.find(\"https://\")\n",
    "  valid_auth = [\"GeoTrust\", \"GoDaddy\", \"Network Solutions\", \"Thawte\", \"Comodo\", \"Doster\" , \"VeriSign\", \"LinkedIn\", \"Sectigo\",\n",
    "                \"Symantec\", \"DigiCert\", \"Network Solutions\", \"RapidSSLonline\", \"SSL.com\", \"Entrust Datacard\", \"Google\", \"Facebook\"]\n",
    "  \n",
    "  cmd = \"curl -vvI \" + url\n",
    "\n",
    "  stdout = Popen(cmd, shell=True, stderr=PIPE, env={}).stderr\n",
    "  output = stdout.read()\n",
    "  std_out = output.decode('UTF-8')\n",
    "  index = std_out.find(\"O=\")\n",
    "\n",
    "  split = std_out[index+2:]\n",
    "  index_sp = split.find(\" \")\n",
    "  cur = split[:index_sp]\n",
    "  \n",
    "  index_sp = cur.find(\",\")\n",
    "  if index_sp!=-1:\n",
    "    cur = cur[:index_sp]\n",
    "  print(cur)\n",
    "  label = -1\n",
    "  if cur in valid_auth and index_https!=-1:\n",
    "    label = 1\n",
    "  \n",
    "  return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_submit_to_email(url):\n",
    "  try:\n",
    "    html_content = requests.get(url).text\n",
    "    soup = BeautifulSoup(html_content, \"lxml\")\n",
    "    form_opt = str(soup.form)\n",
    "    idx = form_opt.find(\"mail()\")\n",
    "    if idx == -1:\n",
    "      idx = form_opt.find(\"mailto:\")\n",
    "\n",
    "    if idx == -1:\n",
    "      return 1\n",
    "    return -1\n",
    "  except:\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def existenceoftoken(u):\n",
    "    ix = u.find(\"//https\")\n",
    "    if(ix==-1):\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import tldextract\n",
    "import whois\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "def dregisterlen(u):\n",
    "    extract_res = tldextract.extract(u)\n",
    "    ul = extract_res.domain + \".\" + extract_res.suffix\n",
    "\n",
    "    try:\n",
    "        wres = whois.whois(u)\n",
    "        f = wres[\"creation_date\"][0]\n",
    "        s = wres[\"expiration_date\"][0]\n",
    "        if(s>f+relativedelta(months=+12)):\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "    except:\n",
    "        return -1\n",
    "    \n",
    "\n",
    "\n",
    "print(dregisterlen(\"https://www.linkedin.com/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def sfh(u):\n",
    "    try:\n",
    "        programhtml = requests.get(u).text\n",
    "        s = BeautifulSoup(programhtml,\"lxml\")\n",
    "        f = str(s.form)\n",
    "        ac = f.find(\"action\")\n",
    "        if(ac!=-1):\n",
    "            i1 = f[ac:].find(\">\")\n",
    "            u1 = f[ac+8:i1-1]\n",
    "            if(u1==\"\" or u1==\"about:blank\"):\n",
    "                return -1\n",
    "            er1 = tldextract.extract(u)\n",
    "            upage = er1.domain\n",
    "            erl2 = tldextract.extract(u1)\n",
    "            usfh = erl2.domain\n",
    "            if upage in usfh:\n",
    "                return 1\n",
    "            return 0\n",
    "        else:\n",
    "            #check this point\n",
    "            return 1\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "\n",
    "sfh(\"https://www.linkedin.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tags(u):\n",
    "  try:\n",
    "    programhtml = requests.get(u).text\n",
    "    s = BeautifulSoup(programhtml,\"lxml\")\n",
    "    mtags = s.find_all('Meta')\n",
    "    ud = tldextract.extract(u)\n",
    "    upage = ud.domain\n",
    "    mcount = 0\n",
    "    for i in mtags:\n",
    "        u1 = i['href']\n",
    "        currpage = tldextract.extract(u1)\n",
    "        u1page = currpage.domain\n",
    "        if currpage not in u1page:\n",
    "            mcount+=1\n",
    "    scount = 0\n",
    "    stags = s.find_all('Script')\n",
    "    for j in stags:\n",
    "        u1 = j['href']\n",
    "        currpage = tldextract.extract(u1)\n",
    "        u1page = currpage.domain\n",
    "        if currpage not in u1page:\n",
    "            scount+=1\n",
    "    lcount = 0\n",
    "    ltags = s.find_all('Link')\n",
    "    for k in ltags:\n",
    "        u1 = k['href']\n",
    "        currpage = tldextract.extract(u1)\n",
    "        u1page = currpage.domain\n",
    "        if currpage not in u1page:\n",
    "            lcount+=1\n",
    "    percmtag = 0\n",
    "    percstag = 0\n",
    "    percltag = 0\n",
    "\n",
    "    if len(mtags) != 0:\n",
    "      percmtag = (mcount*100)//len(mtags)\n",
    "    if len(stags) != 0:\n",
    "      percstag = (scount*100)//len(stags)\n",
    "    if len(ltags) != 0:\n",
    "      percltag = (lcount*100)//len(ltags)\n",
    "      \n",
    "    if(percmtag+percstag+percltag<17):\n",
    "        return 1\n",
    "    elif(percmtag+percstag+percltag<=81):\n",
    "        return 0\n",
    "    return -1\n",
    "  except:\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(tags(\"https://www.linkedin.com\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_validator(url):\n",
    "    try:\n",
    "        result = urlparse(url)\n",
    "        return all([result.scheme, result.netloc, result.path])\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_validator(\"http://www.linkedin.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redirect(url):\n",
    "  opt = Popen([\"sh\", \"/red.sh\", url], stdout=PIPE).communicate()[0]\n",
    "  opt = opt.decode('utf-8')\n",
    "  opt = opt.split(\"\\n\")\n",
    "  new = []\n",
    "  for i in opt:\n",
    "    i = i.replace(\"\\r\", \" \")\n",
    "    new.extend(i.split(\" \"))\n",
    "  count = 0\n",
    "  for i in new:\n",
    "    if i.isdigit():\n",
    "      conv = int(i)\n",
    "      if conv > 300 and conv<310:\n",
    "        count += 1\n",
    "        \n",
    "  last_url = None\n",
    "  for i in new[::-1]:\n",
    "    if url_validator(i):\n",
    "      last_url = i\n",
    "      break\n",
    "\n",
    "  if (count<=1):\n",
    "    return 1, last_url\n",
    "  elif count>=2 and count <4:\n",
    "    return 0, last_url\n",
    "  return -1, last_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, None)\n"
     ]
    }
   ],
   "source": [
    "print(redirect(\"https://oxify.me/tuT2y\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pagerank(url):\n",
    "  pageRankApi = \"owwo48o808sw8w4ccck488kw00go8kgwg4s0ck8w\"\n",
    "  extract_res = tldextract.extract(url)\n",
    "  url_ref = extract_res.domain + \".\" + extract_res.suffix\n",
    "  headers = {'API-OPR': pageRankApi}\n",
    "  domain = url_ref\n",
    "  req_url = 'https://openpagerank.com/api/v1.0/getPageRank?domains%5B0%5D=' + domain\n",
    "  request = requests.get(req_url, headers=headers)\n",
    "  result = request.json()\n",
    "  value = result['response'][0]['page_rank_decimal']\n",
    "  if type(value) == str:\n",
    "    value = 0\n",
    "\n",
    "  if value < 2:\n",
    "    return -1\n",
    "  return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pagerank(\"http://www.linkedin.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_web_traffic(url):\n",
    "  try:\n",
    "    extract_res = tldextract.extract(url)\n",
    "    url_ref = extract_res.domain + \".\" + extract_res.suffix\n",
    "    html_content = requests.get(\"https://www.alexa.com/siteinfo/\" + url_ref).text\n",
    "    soup = BeautifulSoup(html_content, \"lxml\")\n",
    "    value = str(soup.find('div', {'class': \"rankmini-rank\"}))[42:].split(\"\\n\")[0].replace(\",\", \"\")\n",
    "\n",
    "    if not value.isdigit():\n",
    "      return -1\n",
    "\n",
    "    value = int(value)\n",
    "    print(value)\n",
    "    if value < 100000:\n",
    "      return 1\n",
    "    return 0\n",
    "  except:\n",
    "    return -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_web_traffic(\"https://www.linkedin.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dns_record(url):\n",
    "  extract_res = tldextract.extract(url)\n",
    "  url_ref = extract_res.domain + \".\" + extract_res.suffix\n",
    "  try:\n",
    "    whois_res = whois.whois(url)\n",
    "    return 1\n",
    "  except:\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"google.com\"\n",
    "check_dns_record(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_age_of_domain(url):\n",
    "  extract_res = tldextract.extract(url)\n",
    "  url_ref = extract_res.domain + \".\" + extract_res.suffix\n",
    "  try:\n",
    "    whois_res = whois.whois(url)\n",
    "    if datetime.datetime.now() > whois_res[\"creation_date\"][0] + relativedelta(months=+6):\n",
    "      return 1\n",
    "    else:\n",
    "      return -1\n",
    "  except:\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_iframe(url):\n",
    "  try:\n",
    "    html_content = requests.get(url).text\n",
    "    soup = BeautifulSoup(html_content, \"lxml\")\n",
    "    if str(soup.iframe).lower().find(\"frameborder\") == -1:\n",
    "      return 1\n",
    "    return -1\n",
    "  except:\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_iframe(\"https://www.linkedin.com0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_rightclick(url):\n",
    "  try:\n",
    "    html_content = requests.get(url).text\n",
    "    soup = BeautifulSoup(html_content, \"lxml\")\n",
    "    if str(soup).lower().find(\"preventdefault()\") != -1:\n",
    "      return -1\n",
    "    elif str(soup).lower().find(\"event.button==2\") != -1:\n",
    "      return -1\n",
    "    elif str(soup).lower().find(\"event.button == 2\") != -1:\n",
    "      return -1\n",
    "    return 1\n",
    "  except:\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_onmouseover(url):\n",
    "  try:\n",
    "    html_content = requests.get(url).text\n",
    "  except:\n",
    "    return -1\n",
    "  soup = BeautifulSoup(html_content, \"lxml\")\n",
    "  if str(soup).lower().find('onmouseover=\"window.status') != -1:\n",
    "    return -1\n",
    "  return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import favicon\n",
    "def check_favicon(url):\n",
    "  try: \n",
    "    extract_res = tldextract.extract(url)\n",
    "    url_ref = extract_res.domain\n",
    "    favs = favicon.get(url)\n",
    "    print(favs)\n",
    "    match = 0\n",
    "    for favi in favs:\n",
    "      url2 = favi.url\n",
    "      extract_res = tldextract.extract(url2)\n",
    "      url_ref2 = extract_res.domain\n",
    "\n",
    "      if url_ref in url_ref2:\n",
    "        match += 1\n",
    "\n",
    "    if match >= len(favs)/2:\n",
    "      return 1\n",
    "    return -1\n",
    "  except:\n",
    "    return -1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_request_URL(url):\n",
    "  try:\n",
    "    extract_res = tldextract.extract(url)\n",
    "    url_ref = extract_res.domain\n",
    "\n",
    "    command_stdout = Popen(['curl', 'https://api.hackertarget.com/pagelinks/?q=' + url], stdout=PIPE).communicate()[0]\n",
    "    links = command_stdout.decode('utf-8').split(\"\\n\")\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for link in links:\n",
    "      extract_res = tldextract.extract(link)\n",
    "      url_ref2 = extract_res.domain\n",
    "\n",
    "      if url_ref not in url_ref2:\n",
    "        count += 1\n",
    "\n",
    "    count /= len(links)\n",
    "\n",
    "    if count < 0.22:\n",
    "      return 1\n",
    "    elif count < 0.61:\n",
    "      return 0\n",
    "    else:\n",
    "      return -1\n",
    "  except:\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    }
   ],
   "source": [
    "print(check_request_URL(\"https://www.linkedin.com0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_URL_of_anchor(url):\n",
    "  try:\n",
    "    extract_res = tldextract.extract(url)\n",
    "    url_ref = extract_res.domain\n",
    "    html_content = requests.get(url).text\n",
    "    soup = BeautifulSoup(html_content, \"lxml\")\n",
    "    a_tags = soup.find_all('a')\n",
    "\n",
    "    if len(a_tags) == 0:\n",
    "      return 1\n",
    "\n",
    "    invalid = ['#', '#content', '#skip', 'JavaScript::void(0)']\n",
    "    bad_count = 0\n",
    "    for t in a_tags:\n",
    "      link = t['href']\n",
    "\n",
    "      if link in invalid:\n",
    "        bad_count += 1\n",
    "\n",
    "      if url_validator(link):\n",
    "        extract_res = tldextract.extract(link)\n",
    "        url_ref2 = extract_res.domain\n",
    "\n",
    "        if url_ref not in url_ref2:\n",
    "          bad_count += 1\n",
    "\n",
    "    bad_count /= len(a_tags)\n",
    "\n",
    "    if bad_count < 0.31:\n",
    "      return 1\n",
    "    elif bad_count <= 0.67:\n",
    "      return 0\n",
    "    return -1\n",
    "  except:\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_statistical_report(url):\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(url):\n",
    "  features_extracted = [0]*25\n",
    "  phStatus, expanded = check_for_shortened_url(url)\n",
    "  features_extracted[2] = phStatus\n",
    "  phStatus, last_url = redirect(url)\n",
    "  features_extracted[16] = phStatus\n",
    "  if expanded is not None:\n",
    "    if len(expanded) >= len(url):\n",
    "      url = expanded\n",
    "\n",
    "  if last_url is not None:\n",
    "    if len(last_url) > len(url):\n",
    "      url = last_url\n",
    "  print(\"before URL\")\n",
    "  print(url)\n",
    "  count = 1\n",
    "  features_extracted[0] = to_find_having_ip_add(url)\n",
    "  count+=1\n",
    "  print(count)\n",
    "  features_extracted[1] = to_find_url_len(url)\n",
    "  count+=1\n",
    "  print(count)\n",
    "  features_extracted[3]  = to_find_at(url)\n",
    "  count+=1\n",
    "  print(count)\n",
    "  features_extracted[4] = to_find_redirect(url)\n",
    "  count+=1\n",
    "  print(count)\n",
    "  features_extracted[5] = to_find_prefix(url)\n",
    "  count+=1\n",
    "  print(count)\n",
    "  features_extracted[6] = to_find_multi_domains(url)\n",
    "  count+=1\n",
    "  print(count)\n",
    "  features_extracted[7] = to_find_authority(url)\n",
    "  count+=1\n",
    "  print(count)\n",
    "  features_extracted[8] = dregisterlen(url)\n",
    "  count+=1\n",
    "  print(count)\n",
    "  features_extracted[9] =  -1 #check_favicon(url)\n",
    "  count+=1\n",
    "  print(count)\n",
    "  features_extracted[10] = existenceoftoken(url)\n",
    "  count+=1\n",
    "  print(count)\n",
    "  features_extracted[11] = check_request_URL(url)\n",
    "  count+=1\n",
    "  print(count)\n",
    "  features_extracted[12] = check_URL_of_anchor(url)\n",
    "  count+=1\n",
    "  print(count)\n",
    "  features_extracted[13] = tags(url)\n",
    "  count+=1\n",
    "  print(count)\n",
    "  features_extracted[14] = sfh(url)\n",
    "  count+=1\n",
    "  print(count)\n",
    "  features_extracted[15] = check_submit_to_email(url)\n",
    "  count+=1\n",
    "  print(count)\n",
    "  features_extracted[17] = check_onmouseover(url)\n",
    "  count+=1\n",
    "  print(count)\n",
    "  features_extracted[18] = check_rightclick(url)\n",
    "  count+=1\n",
    "  print(count)\n",
    "  features_extracted[19] = check_iframe(url)\n",
    "  count+=1\n",
    "  print(count)\n",
    "  features_extracted[20] = check_age_of_domain(url)\n",
    "  count+=1\n",
    "  print(count)\n",
    "  features_extracted[21] = check_dns_record(url)\n",
    "  count+=1\n",
    "  print(count)\n",
    "  features_extracted[22] = 0 #check_web_traffic(url)\n",
    "  count+=1\n",
    "  print(count)\n",
    "  features_extracted[23] = get_pagerank(url)\n",
    "  count+=1\n",
    "  print(count)\n",
    "  features_extracted[24] = check_statistical_report(url)\n",
    "  count+=1\n",
    "  print(count)\n",
    "\n",
    "  return features_extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before URL\n",
      "https://www.instagram.com\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Facebook\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 0, 1, 0]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_features(\"https://www.instagram.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting import-ipynb\n",
      "  Downloading import-ipynb-0.1.3.tar.gz (4.0 kB)\n",
      "Building wheels for collected packages: import-ipynb\n",
      "  Building wheel for import-ipynb (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for import-ipynb: filename=import_ipynb-0.1.3-py3-none-any.whl size=2975 sha256=7a290d766878a1c3846244331a65382fc6136c62105ca1a1db2a6287fafac479\n",
      "  Stored in directory: /home/sumit/.cache/pip/wheels/06/7e/ad/1cb03e935234186825cefc7e2c8f3451b4f654b5bc72232a7b\n",
      "Successfully built import-ipynb\n",
      "Installing collected packages: import-ipynb\n",
      "Successfully installed import-ipynb-0.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install import-ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}